{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading required data\n",
    "import pickle as pkl\n",
    "\n",
    "with open(\"../resources/annotated_data.pickle\",\"rb\") as pkl_in:\n",
    "    data = pkl.load(pkl_in)\n",
    "    text = pkl.load(pkl_in)\n",
    "    lu = pkl.load(pkl_in)\n",
    "\n",
    "with open(\"../resources/sentence_features_en.pickle\", \"rb\") as pkl_en:\n",
    "    en_fn_features = pkl.load(pkl_en)\n",
    "    en_fe_features = pkl.load(pkl_en)\n",
    "    total_en = pkl.load(pkl_en)\n",
    "    en_lu_ft = pkl.load(pkl_en)\n",
    "    en_fe_lu_ft = pkl.load(pkl_en)\n",
    "    en_lu_bert = pkl.load(pkl_en)\n",
    "    \n",
    "with open(\"../resources/sentence_features_pt.pickle\", \"rb\") as pkl_pt:\n",
    "    pt_fn_features = pkl.load(pkl_pt)\n",
    "    pt_fe_features = pkl.load(pkl_pt)\n",
    "    total_pt = pkl.load(pkl_pt)\n",
    "    pt_lu_ft = pkl.load(pkl_pt)\n",
    "    pt_fe_lu_ft = pkl.load(pkl_pt)\n",
    "    pt_lu_bert = pkl.load(pkl_pt)\n",
    "    \n",
    "with open(\"../resources/sentence_features_de.pickle\", \"rb\") as pkl_de:\n",
    "    de_fn_features = pkl.load(pkl_de)\n",
    "    de_fe_features = pkl.load(pkl_de)\n",
    "    total_de = pkl.load(pkl_de)\n",
    "    de_lu_ft = pkl.load(pkl_de)\n",
    "    de_fe_lu_ft = pkl.load(pkl_de)\n",
    "    de_lu_bert = pkl.load(pkl_de)\n",
    "\n",
    "with open(\"../resources/bert_embeddings.pickle\",\"rb\") as pkl_in:\n",
    "    lu_dict_en = pkl.load(pkl_in)\n",
    "    sent_dict_en = pkl.load(pkl_in)\n",
    "    lu_dict_pt = pkl.load(pkl_in)\n",
    "    sent_dict_pt = pkl.load(pkl_in)\n",
    "    lu_dict_de = pkl.load(pkl_in)\n",
    "    sent_dict_de = pkl.load(pkl_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_score(df):\n",
    "    df = df.fillna(0) #fills NaN values with 0s, #let us give annotator confusion a score of 0\n",
    "    df = df.drop([0], axis = 1) #drops first column which contains annotator ID\n",
    "    sent = df.iloc[0] #gets first row\n",
    "    sent = list(sent)\n",
    "    df = df.drop([0], axis = 0) #drops first column which contains the sentence pairs -- shifted to row 0 because we chose header = None \n",
    "    \n",
    "    #calculating scores of each sentence pair by taking weighted average of the different scores given by 7 annotators\n",
    "    scores = {}\n",
    "    for i in range(len(df.columns) + 1):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        df[i] = pd.to_numeric(df[i])\n",
    "        key = sent[i - 1].split('[')[0]\n",
    "        scores[key] = (df[i] - 1).sum()/((4 - 1)*len(df))\n",
    "        \n",
    "    return scores, sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt') --- uncomment when running the code for the first time\n",
    "def key_generator(lang1, lang2, merged_sent, sent_dict_1, sent_dict_2):\n",
    "    \n",
    "    key1 = ''\n",
    "    key2 = ''\n",
    "    \n",
    "    sents = merged_sent.split('  ')\n",
    "    #print(\"Sents : \",sents)\n",
    "    \n",
    "    if lang1 == 'en':\n",
    "        try:\n",
    "            key1 = str(sent_dict_1[sents[0].split('EN: ')[1].strip()]) + '_en'\n",
    "\n",
    "        #if multiple sentences of English correspond to a single sentence of Lang2\n",
    "        except:\n",
    "            temp = sents[0].split(':')[1].strip()\n",
    "            sent_text = nltk.sent_tokenize(temp)\n",
    "\n",
    "            #get keys of the multiple sentences\n",
    "            for j in range(len(sent_text)):\n",
    "                if key1 == '':\n",
    "                    key1 = str(sent_dict_1[sent_text[j]]) + '_en'\n",
    "                else:\n",
    "                    key1 = key1 + '-' + str(sent_dict_1[sent_text[j]]) + '_en'\n",
    "                    \n",
    "        #print(key1)\n",
    "    \n",
    "    if lang2 == 'pt':\n",
    "        try:\n",
    "            key2 = str(sent_dict_2[sents[1].split('PT: ')[1].strip()]) + '_pt'\n",
    "\n",
    "        #if multiple sentences of Portuguese correspond to a single sentence of English\n",
    "        except:\n",
    "            temp = sents[1].split(':')[1].strip()\n",
    "\n",
    "            #nltk tokenizer doesn't split sentences based on '...' , special case handling\n",
    "            if '...' in temp:\n",
    "                sent_text = temp.split('...')\n",
    "\n",
    "                #when the split is happening, after splitting, there will always be even no. of items. The i%2 == 0 th item will be having the '...' sign based on which the split happened.\n",
    "                for k in range(len(sent_text)):\n",
    "                    if k % 2 == 0 :\n",
    "                        sent_text[k] = sent_text[k] + '...'\n",
    "            else:\n",
    "                sent_text = nltk.sent_tokenize(temp)\n",
    "\n",
    "            #get keys of multiple sentences\n",
    "            for j in range(len(sent_text)):\n",
    "                if key2 == '':\n",
    "                    key2 = str(sent_dict_2[sent_text[j].strip()]) + '_pt'\n",
    "                else:\n",
    "                    key2 = key2 + '-' + str(sent_dict_2[sent_text[j].strip()]) + '_pt'\n",
    "                    \n",
    "    elif lang2 == 'de':\n",
    "        try:\n",
    "            key2 = str(sent_dict_2[sents[1].split('DE:')[1].strip().replace(u'\\xa0', u' ')]) + '_de'\n",
    "\n",
    "        #if multiple sentences of English correspond to a single sentence of Lang2\n",
    "        except:\n",
    "            temp = sents[1].split(':')[1].strip()\n",
    "            temp = temp.replace(u'\\xa0', u' ')\n",
    "            sent_text = nltk.sent_tokenize(temp)\n",
    "\n",
    "            #get keys of the multiple sentences\n",
    "            for j in range(len(sent_text)):\n",
    "                if key2 == '':\n",
    "                    key2 = str(sent_dict_2[sent_text[j]]) + '_de'\n",
    "                else:\n",
    "                    key2 = key2 + '-' + str(sent_dict_2[sent_text[j]]) + '_de'\n",
    "    return key1, key2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating format for regressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "def get_features(lang, sent_dict_1, sent_dict_2, aligned, score_choice):\n",
    "    \n",
    "    data_final = {}\n",
    "    \n",
    "    if lang == 'en-pt':\n",
    "        \n",
    "        df_pt = pd.read_csv('../resources/results-survey-en-pt.csv', header = None)\n",
    "        k1 = {v : k for k, v in sent_dict_1.items()}  #sentid : sentence\n",
    "        k2 = {v : k for k, v in sent_dict_2.items()}\n",
    "                \n",
    "        if score_choice == 1:\n",
    "            scores, key_list = get_score(df_pt)\n",
    "            key_list_extra = ['EN: ' + k1[aligned[i][0]] + '  ' + 'PT: ' + k2[aligned[i][1]] for i in range(len(aligned))]\n",
    "            key_list = key_list + key_list_extra\n",
    "        else:\n",
    "            k1 = list(sent_dict_1.keys())\n",
    "            k2 = list(sent_dict_2.keys())\n",
    "            key_list = ['EN: ' + k1[i] + '  ' + 'PT: ' + k2[i] for i in range(len(k1))]\n",
    "        \n",
    "        \n",
    "        for i in range(len(key_list)):\n",
    "            #print(key_list[i])\n",
    "            if score_choice == 1:\n",
    "                try:\n",
    "                    key_list[i] = key_list[i].split('[')[0]\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "            en_key, pt_key = key_generator('en', 'pt', key_list[i], sent_dict_1, sent_dict_2)\n",
    "            #print(en_key, pt_key)\n",
    "            gen_key = en_key + '--' + pt_key\n",
    "            #print(gen_key)\n",
    "            data_final[gen_key] = {}\n",
    "            \n",
    "            temp1 = en_key.split('-')\n",
    "            temp2 = pt_key.split('-')\n",
    "            #print(en_key, pt_key)\n",
    "            #print(len(temp1),temp1, temp2)\n",
    "            \n",
    "            f1 = []\n",
    "            f2 = []\n",
    "            f3 = []\n",
    "            f4 = []\n",
    "            f5 = []\n",
    "            f6 = []\n",
    "            f7 = []\n",
    "            f8 = []\n",
    "            f9 = []\n",
    "            f10 = []\n",
    "            \n",
    "            #if sentence mapping is 1:1\n",
    "            if len(temp1) == 1:\n",
    "                data_final[gen_key]['lang1_fn_ft'] = en_fn_features[en_key]\n",
    "                data_final[gen_key]['lang1_fe_ft'] = en_fe_features[en_key]\n",
    "                data_final[gen_key]['lang1_total_ft'] = total_en[en_key]\n",
    "                data_final[gen_key]['lang1_lu_ft'] = en_lu_ft[en_key]\n",
    "                data_final[gen_key]['lang1_fe_lu_ft'] = en_fe_lu_ft[en_key]\n",
    "                \n",
    "            #if sentence mapping is n:1\n",
    "            else:\n",
    "\n",
    "                for j in range(len(temp1) - 1):\n",
    "                    print('English key - ', temp1[j])\n",
    "                    f1.append(en_fn_features[temp1[j]])\n",
    "                    f3.append(en_fe_features[temp1[j]])\n",
    "                    f5.append(total_en[temp1[j]])\n",
    "                    f7.append(en_lu_ft[temp1[j]])\n",
    "                    f9.append(en_fe_lu_ft[temp1[j]])\n",
    "\n",
    "                data_final[gen_key]['lang1_fn_ft'] = np.average(f1, axis = 0)\n",
    "                data_final[gen_key]['lang1_fe_ft'] = np.average(f3, axis = 0)\n",
    "                data_final[gen_key]['lang1_total_ft'] = np.average(f5, axis = 0)\n",
    "                data_final[gen_key]['lang1_lu_ft'] = np.average(f7, axis = 0)\n",
    "                data_final[gen_key]['lang1_fe_lu_ft'] = np.average(f9, axis = 0)\n",
    "\n",
    "            if len(temp2) == 1:\n",
    "                data_final[gen_key]['lang2_fn_ft'] = pt_fn_features[pt_key]\n",
    "                data_final[gen_key]['lang2_fe_ft'] = pt_fe_features[pt_key]\n",
    "                data_final[gen_key]['lang2_total_ft'] = total_pt[pt_key]\n",
    "                data_final[gen_key]['lang2_lu_ft'] = pt_lu_ft[pt_key]\n",
    "                data_final[gen_key]['lang2_fe_lu_ft'] = pt_fe_lu_ft[pt_key]\n",
    "                \n",
    "            else:\n",
    "                for j in range(len(temp2) - 1):\n",
    "\n",
    "                    f2.append(pt_fn_features[temp2[j]])\n",
    "                    f4.append(pt_fe_features[temp2[j]])\n",
    "                    f6.append(total_pt[temp2[j]])\n",
    "                    f8.append(pt_lu_ft[temp2[j]])\n",
    "                    f10.append(pt_fe_lu_ft[temp2[j]])         \n",
    "\n",
    "                data_final[gen_key]['lang2_fn_ft'] = np.average(f2, axis = 0)\n",
    "                data_final[gen_key]['lang2_fe_ft'] = np.average(f4, axis = 0)\n",
    "                data_final[gen_key]['lang2_total_ft'] = np.average(f6, axis = 0)\n",
    "                data_final[gen_key]['lang2_lu_ft'] = np.average(f8, axis = 0)\n",
    "                data_final[gen_key]['lang2_fe_lu_ft'] = np.average(f10, axis = 0)\n",
    "\n",
    "            if score_choice == 1 :\n",
    "                try:\n",
    "                    data_final[gen_key]['y'] = scores[key_list[i].split('[')[0]]\n",
    "                except:\n",
    "                    data_final[gen_key]['y'] = 10.0    #a dummy score of 10 is assigned to sentences with no human annotated score\n",
    "            else:\n",
    "                data_final[gen_key]['y'] = 0.0\n",
    "\n",
    "            #print(df_pt)\n",
    "    \n",
    "    elif lang == 'en-de':\n",
    "        \n",
    "        df_de = pd.read_csv('../resources/results-survey-en-de.csv', header = None)\n",
    "        k1 = {v : k for k, v in sent_dict_1.items()}\n",
    "        k2 = {v : k for k, v in sent_dict_2.items()}\n",
    "        \n",
    "        if score_choice == 1:\n",
    "            scores, key_list = get_score(df_de)\n",
    "            key_list_extra = ['EN: ' + k1[aligned[i][0]] + '  ' + 'DE: ' + k2[aligned[i][1]] for i in range(len(aligned))]\n",
    "            key_list = key_list + key_list_extra\n",
    "        else:\n",
    "            k1 = list(sent_dict_1.keys())\n",
    "            k2 = list(sent_dict_2.keys())\n",
    "            key_list = ['EN: ' + k1[i] + '  ' + 'DE: ' + k2[i] for i in range(len(k1))]\n",
    "        \n",
    "        for i in range(len(key_list)):\n",
    "            #print(key_list[i])\n",
    "            en_key, de_key = key_generator('en', 'de', key_list[i], sent_dict_1, sent_dict_2)    \n",
    "            gen_key = en_key + '--' + de_key\n",
    "            data_final[gen_key] = {}\n",
    "\n",
    "            temp1 = en_key.split('-')\n",
    "            temp2 = de_key.split('-')\n",
    "            \n",
    "            f1 = []\n",
    "            f2 = []\n",
    "            f3 = []\n",
    "            f4 = []\n",
    "            f5 = []\n",
    "            f6 = []\n",
    "            f7 = []\n",
    "            f8 = []\n",
    "            f9 = []\n",
    "            f10 = []\n",
    "            \n",
    "            if len(temp1) == 1:\n",
    "                data_final[gen_key]['lang1_fn_ft'] = en_fn_features[en_key]\n",
    "                data_final[gen_key]['lang1_fe_ft'] = en_fe_features[en_key]\n",
    "                data_final[gen_key]['lang1_total_ft'] = total_en[en_key]\n",
    "                data_final[gen_key]['lang1_lu_ft'] = en_lu_ft[en_key]\n",
    "                data_final[gen_key]['lang1_fe_lu_ft'] = en_fe_lu_ft[en_key]\n",
    "                \n",
    "            else:\n",
    "                for j in range(len(temp1) - 1):\n",
    "\n",
    "                    f1.append(en_fn_features[temp1[j]])\n",
    "                    f3.append(en_fe_features[temp1[j]])\n",
    "                    f5.append(total_en[temp1[j]])\n",
    "                    f7.append(en_lu_ft[temp1[j]])\n",
    "                    f9.append(en_fe_lu_ft[temp1[j]])\n",
    "\n",
    "                data_final[gen_key]['lang1_fn_ft'] = np.average(f1, axis = 0)\n",
    "                data_final[gen_key]['lang1_fe_ft'] = np.average(f3, axis = 0)\n",
    "                data_final[gen_key]['lang1_total_ft'] = np.average(f5, axis = 0)\n",
    "                data_final[gen_key]['lang1_lu_ft'] = np.average(f7, axis = 0)\n",
    "                data_final[gen_key]['lang1_fe_lu_ft'] = np.average(f9, axis = 0)\n",
    "\n",
    "\n",
    "            if len(temp2) == 1:\n",
    "                data_final[gen_key]['lang2_fn_ft'] = de_fn_features[de_key]\n",
    "                data_final[gen_key]['lang2_fe_ft'] = de_fe_features[de_key]\n",
    "                data_final[gen_key]['lang2_total_ft'] = total_de[de_key]\n",
    "                data_final[gen_key]['lang2_lu_ft'] = de_lu_ft[de_key]\n",
    "                data_final[gen_key]['lang2_fe_lu_ft'] = de_fe_lu_ft[de_key]\n",
    "                \n",
    "            else:\n",
    "                for j in range(len(temp2) - 1):\n",
    "\n",
    "                    f2.append(de_fn_features[temp2[j]])\n",
    "                    f4.append(de_fe_features[temp2[j]])\n",
    "                    f6.append(total_pt[temp2[j]])\n",
    "                    f8.append(de_lu_ft[temp2[j]])\n",
    "                    f10.append(de_fe_lu_ft[temp2[j]])         \n",
    "\n",
    "                data_final[gen_key]['lang2_fn_ft'] = np.average(f2, axis = 0)\n",
    "                data_final[gen_key]['lang2_fe_ft'] = np.average(f4, axis = 0)\n",
    "                data_final[gen_key]['lang2_total_ft'] = np.average(f6, axis = 0)\n",
    "                data_final[gen_key]['lang2_lu_ft'] = np.average(f8, axis = 0)\n",
    "                data_final[gen_key]['lang2_fe_lu_ft'] = np.average(f10, axis = 0)\n",
    "\n",
    "            if score_choice == 1:\n",
    "                try:\n",
    "                    data_final[gen_key]['y'] = scores[key_list[i]]\n",
    "                except:\n",
    "                    data_final[gen_key]['y'] = 10.0\n",
    "            else:\n",
    "                data_final[gen_key]['y'] = 0.0\n",
    "\n",
    "        #print(df_de)\n",
    "\n",
    "    return data_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating format for regressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "def get_features_bert(lang, sent_dict_1, sent_dict_2, aligned, score_choice):\n",
    "    \n",
    "    data_final = {}\n",
    "    sent_index_1 = {str(v) : k for k, v in sent_dict_1.items()}\n",
    "    sent_index_2 = {str(v) : k for k, v in sent_dict_2.items()}\n",
    "    \n",
    "    if lang == 'en-pt':\n",
    "        \n",
    "        df_pt = pd.read_csv('../resources/results-survey-en-pt.csv', header = None)\n",
    "        k1 = {v : k for k, v in sent_dict_1.items()}  #sentid : sentence\n",
    "        k2 = {v : k for k, v in sent_dict_2.items()}\n",
    "                \n",
    "        if score_choice == 1:\n",
    "            scores, key_list = get_score(df_pt)\n",
    "            key_list_extra = ['EN: ' + k1[aligned[i][0]] + '  ' + 'PT: ' + k2[aligned[i][1]] for i in range(len(aligned))]\n",
    "            key_list = key_list + key_list_extra\n",
    "        else:\n",
    "            k1 = list(sent_dict_1.keys())\n",
    "            k2 = list(sent_dict_2.keys())\n",
    "            key_list = ['EN: ' + k1[i] + '  ' + 'PT: ' + k2[i] for i in range(len(k1))]\n",
    "        \n",
    "        #print(key_list)\n",
    "        for i in range(len(key_list)):\n",
    "            \n",
    "            if score_choice == 1:\n",
    "                try:\n",
    "                    key_list[i] = key_list[i].split('[')[0]\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            en_key, pt_key = key_generator('en', 'pt', key_list[i], sent_dict_1, sent_dict_2)\n",
    "            #print(en_key, pt_key)\n",
    "            gen_key = en_key + '--' + pt_key\n",
    "            #print(gen_key)\n",
    "            data_final[gen_key] = {}\n",
    "            \n",
    "            temp1 = en_key.split('-')\n",
    "            temp2 = pt_key.split('-')\n",
    "            #print(en_key, pt_key)\n",
    "            #print(len(temp1),temp1, temp2)\n",
    "            \n",
    "            f1 = []\n",
    "            f2 = []\n",
    "            f3 = []\n",
    "            f4 = []\n",
    "            f5 = []\n",
    "            f6 = []\n",
    "            f7 = []\n",
    "            f8 = []\n",
    "            #f9 = []\n",
    "            #f10 = []\n",
    "            \n",
    "            #if sentence mapping is 1:1\n",
    "            if len(temp1) == 1:\n",
    "                data_final[gen_key]['lang1_fn_ft'] = en_fn_features[en_key]\n",
    "                data_final[gen_key]['lang1_fe_ft'] = en_fe_features[en_key]\n",
    "                try:\n",
    "                    data_final[gen_key]['lang1_sent_ft'] = sent_dict_en[sent_index_1[en_key.replace('_en','')]][0]\n",
    "                except:\n",
    "                    data_final[gen_key]['lang1_sent_ft'] = np.zeros(768)\n",
    "                data_final[gen_key]['lang1_lu_ft'] = en_lu_bert[en_key]\n",
    "                #data_final[gen_key]['lang1_fe_lu_ft'] = en_fe_lu_ft[en_key]\n",
    "                \n",
    "            #if sentence mapping is n:1\n",
    "            else:\n",
    "\n",
    "                for j in range(len(temp1) - 1):\n",
    "                    print('English key - ', temp1[j])\n",
    "                    f1.append(en_fn_features[temp1[j]])\n",
    "                    f3.append(en_fe_features[temp1[j]])\n",
    "                    try:\n",
    "                        f5.append(sent_dict_en[sent_index_1[temp1[j].replace('_en','')]][0])\n",
    "                    except:\n",
    "                        f5.append(np.zeros(768))\n",
    "                    f7.append(en_lu_bert[temp1[j]])\n",
    "                    #f9.append(en_fe_lu_ft[temp1[j]])\n",
    "\n",
    "                data_final[gen_key]['lang1_fn_ft'] = np.average(f1, axis = 0)\n",
    "                data_final[gen_key]['lang1_fe_ft'] = np.average(f3, axis = 0)\n",
    "                data_final[gen_key]['lang1_sent_ft'] = np.average(f5, axis = 0)\n",
    "                data_final[gen_key]['lang1_lu_ft'] = np.average(f7, axis = 0)\n",
    "                #data_final[gen_key]['lang1_fe_lu_ft'] = np.average(f9, axis = 0)\n",
    "\n",
    "            if len(temp2) == 1:\n",
    "                data_final[gen_key]['lang2_fn_ft'] = pt_fn_features[pt_key]\n",
    "                data_final[gen_key]['lang2_fe_ft'] = pt_fe_features[pt_key]\n",
    "                try:\n",
    "                    data_final[gen_key]['lang2_sent_ft'] = sent_dict_pt[sent_index_2[pt_key.replace('_pt','')]][0]\n",
    "                except:\n",
    "                    data_final[gen_key]['lang2_sent_ft'] = np.zeros(768)\n",
    "                data_final[gen_key]['lang2_lu_ft'] = pt_lu_bert[pt_key]\n",
    "                #data_final[gen_key]['lang2_fe_lu_ft'] = pt_fe_lu_ft[pt_key]\n",
    "                \n",
    "            else:\n",
    "                for j in range(len(temp2) - 1):\n",
    "\n",
    "                    f2.append(pt_fn_features[temp2[j]])\n",
    "                    f4.append(pt_fe_features[temp2[j]])\n",
    "                    try:\n",
    "                        f6.append(sent_dict_pt[sent_index_2[temp2[j].replace('_pt','')]][0])\n",
    "                    except:\n",
    "                        f6.append(np.zeros(768))\n",
    "                    f8.append(pt_lu_bert[temp2[j]])\n",
    "                    #f10.append(pt_fe_lu_ft[temp2[j]])         \n",
    "\n",
    "                data_final[gen_key]['lang2_fn_ft'] = np.average(f2, axis = 0)\n",
    "                data_final[gen_key]['lang2_fe_ft'] = np.average(f4, axis = 0)\n",
    "                data_final[gen_key]['lang2_sent_ft'] = np.average(f6, axis = 0)\n",
    "                data_final[gen_key]['lang2_lu_ft'] = np.average(f8, axis = 0)\n",
    "                #data_final[gen_key]['lang2_fe_lu_ft'] = np.average(f10, axis = 0)\n",
    "\n",
    "            if score_choice == 1 :\n",
    "                try:\n",
    "                    data_final[gen_key]['y'] = scores[key_list[i].split('[')[0]]\n",
    "                except:\n",
    "                    data_final[gen_key]['y'] = 10.0\n",
    "            else:\n",
    "                data_final[gen_key]['y'] = 0.0\n",
    "\n",
    "            #print(df_pt)\n",
    "    \n",
    "    elif lang == 'en-de':\n",
    "        \n",
    "        df_de = pd.read_csv('../resources/results-survey-en-de.csv', header = None)\n",
    "        k1 = {v : k for k, v in sent_dict_1.items()}\n",
    "        k2 = {v : k for k, v in sent_dict_2.items()}\n",
    "        \n",
    "        if score_choice == 1:\n",
    "            scores, key_list = get_score(df_de)\n",
    "            key_list_extra = ['EN: ' + k1[aligned[i][0]] + '  ' + 'DE: ' + k2[aligned[i][1]] for i in range(len(aligned))]\n",
    "            key_list = key_list + key_list_extra\n",
    "        else:\n",
    "            k1 = list(sent_dict_1.keys())\n",
    "            k2 = list(sent_dict_2.keys())\n",
    "            key_list = ['EN: ' + k1[i] + '  ' + 'DE: ' + k2[i] for i in range(len(k1))]\n",
    "        \n",
    "        for i in range(len(key_list)):\n",
    "        \n",
    "            en_key, de_key = key_generator('en', 'de', key_list[i], sent_dict_1, sent_dict_2)    \n",
    "            gen_key = en_key + '--' + de_key\n",
    "            data_final[gen_key] = {}\n",
    "\n",
    "            temp1 = en_key.split('-')\n",
    "            temp2 = de_key.split('-')\n",
    "            \n",
    "            f1 = []\n",
    "            f2 = []\n",
    "            f3 = []\n",
    "            f4 = []\n",
    "            f5 = []\n",
    "            f6 = []\n",
    "            f7 = []\n",
    "            f8 = []\n",
    "            #f9 = []\n",
    "            #f10 = []\n",
    "            \n",
    "            if len(temp1) == 1:\n",
    "                data_final[gen_key]['lang1_fn_ft'] = en_fn_features[en_key]\n",
    "                data_final[gen_key]['lang1_fe_ft'] = en_fe_features[en_key]\n",
    "                try:\n",
    "                    data_final[gen_key]['lang1_sent_ft'] = sent_dict_en[sent_index_1[en_key.replace('_en','')]][0]\n",
    "                except:\n",
    "                    data_final[gen_key]['lang1_sent_ft'] = np.zeros(768)\n",
    "                data_final[gen_key]['lang1_lu_ft'] = en_lu_bert[en_key]\n",
    "                #data_final[gen_key]['lang1_fe_lu_ft'] = en_fe_lu_ft[en_key]\n",
    "                \n",
    "            else:\n",
    "                for j in range(len(temp1) - 1):\n",
    "\n",
    "                    f1.append(en_fn_features[temp1[j]])\n",
    "                    f3.append(en_fe_features[temp1[j]])\n",
    "                    try:\n",
    "                        f5.append(sent_dict_en[sent_index_1[temp1[j].replace('_en','')]][0])\n",
    "                    except:\n",
    "                        f5.append(np.zeros(768))\n",
    "                    f7.append(en_lu_bert[temp1[j]])\n",
    "                    #f9.append(en_fe_lu_ft[temp1[j]])\n",
    "\n",
    "                data_final[gen_key]['lang1_fn_ft'] = np.average(f1, axis = 0)\n",
    "                data_final[gen_key]['lang1_fe_ft'] = np.average(f3, axis = 0)\n",
    "                data_final[gen_key]['lang1_sent_ft'] = np.average(f5, axis = 0)\n",
    "                data_final[gen_key]['lang1_lu_ft'] = np.average(f7, axis = 0)\n",
    "                #data_final[gen_key]['lang1_fe_lu_ft'] = np.average(f9, axis = 0)\n",
    "\n",
    "\n",
    "            if len(temp2) == 1:\n",
    "                data_final[gen_key]['lang2_fn_ft'] = de_fn_features[de_key]\n",
    "                data_final[gen_key]['lang2_fe_ft'] = de_fe_features[de_key]\n",
    "                try:\n",
    "                    data_final[gen_key]['lang2_sent_ft'] = sent_dict_de[sent_index_2[de_key.replace('_de','')]][0]\n",
    "                except:\n",
    "                    data_final[gen_key]['lang2_sent_ft'] = np.zeros(768)\n",
    "                data_final[gen_key]['lang2_lu_ft'] = de_lu_bert[de_key]\n",
    "                #data_final[gen_key]['lang2_fe_lu_ft'] = de_fe_lu_ft[de_key]\n",
    "                \n",
    "            else:\n",
    "                for j in range(len(temp2) - 1):\n",
    "\n",
    "                    f2.append(de_fn_features[temp2[j]])\n",
    "                    f4.append(de_fe_features[temp2[j]])\n",
    "                    try:\n",
    "                        f6.append(sent_dict_de[sent_index_2[temp2[j].replace('_de','')]][0])\n",
    "                    except:\n",
    "                        f6.append(np.zeros(768))\n",
    "                    f8.append(de_lu_bert[temp2[j]])\n",
    "                    #f10.append(de_fe_lu_ft[temp2[j]])         \n",
    "\n",
    "                data_final[gen_key]['lang2_fn_ft'] = np.average(f2, axis = 0)\n",
    "                data_final[gen_key]['lang2_fe_ft'] = np.average(f4, axis = 0)\n",
    "                data_final[gen_key]['lang2_sent_ft'] = np.average(f6, axis = 0)\n",
    "                data_final[gen_key]['lang2_lu_ft'] = np.average(f8, axis = 0)\n",
    "                #data_final[gen_key]['lang2_fe_lu_ft'] = np.average(f10, axis = 0)\n",
    "\n",
    "            if score_choice == 1:\n",
    "                try:\n",
    "                    data_final[gen_key]['y'] = scores[key_list[i]]\n",
    "                except:\n",
    "                    data_final[gen_key]['y'] = 10.0\n",
    "            else:\n",
    "                data_final[gen_key]['y'] = 0.0\n",
    "\n",
    "        #print(df_de)\n",
    "\n",
    "    return data_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English key -  1028_en\n",
      "English key -  1031_en\n",
      "English key -  1028_en\n",
      "English key -  1031_en\n"
     ]
    }
   ],
   "source": [
    "#def main():\n",
    "      \n",
    "df_prime = pd.read_csv(\"../resources/en-pt-de.csv\", skiprows = 1, names = [\"En_Id\",\"En_Sentence\",\"Pt_Id\",\"Pt_Sentence\",\"De_Id\",\"De_Sentence\"], encoding = 'utf-8')\n",
    "\n",
    "#English - Portuguese\n",
    "df = pd.read_csv(\"../resources/en-pt.csv\", skiprows = 1, names = [\"En_Id\",\"En_Sentence\",\"Pt_Id\",\"Pt_Sentence\"], encoding = 'utf-8')\n",
    "\n",
    "#getting subsets of the original dataframe\n",
    "df_en = df[[col for col in df.columns if \"Pt_Id\" not in col and 'Pt_Sentence' not in col]]\n",
    "df_pt = df[[col for col in df.columns if \"En_Id\" not in col and 'En_Sentence' not in col]]\n",
    "\n",
    "#keeping the unscored sentences separate\n",
    "aligned_enpt_extra = [(row[\"En_Id\"], row[\"Pt_Id\"]) for index, row in df.iterrows() if row['En_Id'] not in df_prime['En_Id'].values and row['Pt_Id'] not in df_prime['Pt_Id'].values]\n",
    "\n",
    "#Deutsche\n",
    "df = pd.read_csv(\"../resources/en-de.csv\", skiprows = 1, names = [\"En_Id\",\"En_Sentence\",\"De_Id\",\"De_Sentence\"], encoding = 'utf-8')\n",
    "\n",
    "df_de = df[[col for col in df.columns if \"En_Id\" not in col and 'En_Sentence' not in col]]\n",
    "\n",
    "#keeping the unscored sentences separate\n",
    "aligned_ende_extra = [(row[\"En_Id\"], row[\"De_Id\"]) for index, row in df.iterrows() if row['En_Id'] not in df_prime['En_Id'].values and row['De_Id'] not in df_prime['De_Id'].values]\n",
    "\n",
    "#retrieving sentence id from sentences\n",
    "#key = sentence, value = sentence id\n",
    "key_set_en = df_en.set_index('En_Sentence').to_dict()['En_Id']\n",
    "key_set_en = {k.strip() : v for k, v in key_set_en.items()}\n",
    "key_set_pt = df_pt.set_index('Pt_Sentence').to_dict()['Pt_Id']\n",
    "key_set_pt = {k.strip() : v for k, v in key_set_pt.items()}\n",
    "key_set_de = df_de.set_index('De_Sentence').to_dict()['De_Id']\n",
    "\n",
    "# Deutsche script contained \\xa0 which is actually non-breaking space in Latin1 (ISO 8859-1), also chr(160). We replace it with a space.\n",
    "key_set_de = {k.replace(u'\\xa0', u' ') : v for k,v in key_set_de.items()} \n",
    "\n",
    "#using fasttext\n",
    "data_en_pt = get_features('en-pt', key_set_en, key_set_pt, aligned_enpt_extra, 1)  #key_pt contains en as well\n",
    "data_en_de = get_features('en-de', key_set_en, key_set_de, aligned_ende_extra, 1)\n",
    "\n",
    "#using bert\n",
    "data_en_pt_bert = get_features_bert('en-pt', key_set_en, key_set_pt, aligned_enpt_extra, 1)  #key_pt contains en as well\n",
    "data_en_de_bert = get_features_bert('en-de', key_set_en, key_set_de, aligned_ende_extra, 1)\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "#    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate negative samples\n",
    "from random import sample, seed\n",
    "\n",
    "seed(25)\n",
    "\n",
    "neg_samples_en = sample(list(key_set_en), 10) \n",
    "#print(neg_samples_en)\n",
    "neg_samples_pt = sample(list(key_set_pt), 10) \n",
    "#print(neg_samples_pt)\n",
    "neg_samples_de = sample(list(key_set_de), 10) \n",
    "#print(neg_samples_de)\n",
    "\n",
    "neg_set_en = {sent : key_set_en[sent] for sent in neg_samples_en}\n",
    "neg_set_pt = {sent : key_set_pt[sent] for sent in neg_samples_pt}\n",
    "neg_set_de = {sent : key_set_de[sent] for sent in neg_samples_de}\n",
    "\n",
    "neg_en_pt = get_features('en-pt', neg_set_en, neg_set_pt, aligned_enpt_extra, 0)\n",
    "neg_en_de = get_features('en-de', neg_set_en, neg_set_de, aligned_ende_extra, 0)\n",
    "\n",
    "#using bert\n",
    "neg_en_pt_bert = get_features_bert('en-pt', neg_set_en, neg_set_pt, aligned_enpt_extra, 0)  #key_pt contains en as well\n",
    "neg_en_de_bert = get_features_bert('en-de', neg_set_en, neg_set_de, aligned_ende_extra, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282 10\n",
      "292\n",
      "55 10\n",
      "65\n",
      "Original Data Sample size :  337\n",
      "Final Sample size :  357\n"
     ]
    }
   ],
   "source": [
    "#merging data\n",
    "print(len(data_en_pt), len(neg_en_pt))\n",
    "final_data_en_pt = {**data_en_pt, **neg_en_pt}\n",
    "print(len(final_data_en_pt))\n",
    "\n",
    "print(len(data_en_de), len(neg_en_de))\n",
    "final_data_en_de = {**data_en_de, **neg_en_de}\n",
    "print(len({**data_en_de, **neg_en_de}))\n",
    "\n",
    "final_pos_data = {**data_en_pt, **data_en_de}\n",
    "print(\"Original Data Sample size : \", len(final_pos_data))\n",
    "\n",
    "final_data = {**data_en_pt_bert, **neg_en_pt_bert, **data_en_de_bert, **neg_en_de_bert}\n",
    "print(\"Final Sample size : \", len(final_data))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open(\"../resources/final_data.pickle\", \"wb\") as pkl_out:\n",
    "    pkl.dump(data_en_pt, pkl_out)\n",
    "    pkl.dump(data_en_de, pkl_out)\n",
    "    pkl.dump(final_data_en_pt, pkl_out)\n",
    "    pkl.dump(final_data_en_de, pkl_out)\n",
    "    pkl.dump(final_pos_data, pkl_out)\n",
    "    pkl.dump(final_data, pkl_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsoc19",
   "language": "python",
   "name": "gsoc19"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
