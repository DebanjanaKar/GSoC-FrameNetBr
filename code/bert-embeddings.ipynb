{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer, BertModel    #, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "with open(\"../resources/annotated_data.pickle\",\"rb\") as pkl_in:\n",
    "    data = pkl.load(pkl_in)\n",
    "    text = pkl.load(pkl_in)\n",
    "    lu = pkl.load(pkl_in)\n",
    "    pos_tag = pkl.load(pkl_in)\n",
    "    frame_name = pkl.load(pkl_in)\n",
    "    frame_element = pkl.load(pkl_in)\n",
    "    frame_element_lu = pkl.load(pkl_in)\n",
    "    lang = pkl.load(pkl_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_en = text['en']\n",
    "sentences_pt = text['pt']\n",
    "sentences_de = text['de']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentwise_ft(lang, list_):\n",
    "    ft_dict = {}\n",
    "    for i in list_[lang] :\n",
    "        if i[2] not in ft_dict:\n",
    "            ft_dict[i[2]] = {i[1] : i[0]}\n",
    "        else:\n",
    "            ft_dict[i[2]][i[1]] = i[0]\n",
    "            \n",
    "    return ft_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def lu_to_word_mapper(lu_list, sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "\n",
    "    tokens = []\n",
    "    for w in words:\n",
    "        for piece in w.split('-'):\n",
    "            tokens.append(piece)\n",
    "\n",
    "    stem_dict = {ps.stem(w) : w for w in tokens}\n",
    "    lu_to_word = {}\n",
    "    #lu_to_word = {lu : [stem_dict[w] if lu in w or w in lu else stem_dict[w] if lu[:5] in w[:5] else None for w in stem_dict.keys()][0] for lu in lu_list}\n",
    "\n",
    "    for lu in lu_list:\n",
    "        choice = 0\n",
    "        for w in stem_dict.keys():\n",
    "            if len(lu) <= 2 or len(w) <= 2:\n",
    "                if lu == w : \n",
    "                    lu_to_word[lu] = stem_dict[w]\n",
    "            else:\n",
    "                #print(lu, w)\n",
    "                if lu == w :\n",
    "                    lu_to_word[lu] = stem_dict[w]\n",
    "                    choice = 1\n",
    "                elif choice != 1:\n",
    "                    if lu in w or w in lu :\n",
    "                        lu_to_word[lu] = stem_dict[w]\n",
    "\n",
    "                    elif w.lower() in lu or lu in w.lower():\n",
    "                        lu_to_word[lu] = stem_dict[w]\n",
    "\n",
    "                    elif lu[:2] in w.lower()[:2] :\n",
    "                        lu_to_word[lu] = stem_dict[w]\n",
    "\n",
    "                    elif lu not in lu_to_word:\n",
    "                            lu_to_word[lu] = lu\n",
    "\n",
    "    return lu_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_token_embedding(encoded_layers, tokenized_text):\n",
    "    \n",
    "    # Convert the hidden state embeddings into single token vectors\n",
    "\n",
    "    # Holds the list of 12 layer embeddings for each token\n",
    "    # Will have the shape: [# tokens, # layers, # features]\n",
    "    token_embeddings = [] \n",
    "    token_dict = {}\n",
    "    batch_i = 0\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token_i in range(len(tokenized_text)):\n",
    "        \n",
    "        # Holds 12 layers of hidden states for each token \n",
    "        hidden_layers = [] \n",
    "        concatenated_last_4_layers = []\n",
    "\n",
    "        # For each of the 12 layers...\n",
    "        for layer_i in range(len(encoded_layers)):\n",
    "\n",
    "            # Lookup the vector for `token_i` in `layer_i`\n",
    "            vec = encoded_layers[layer_i][batch_i][token_i]\n",
    "\n",
    "            hidden_layers.append(vec)\n",
    "            \n",
    "        token_embeddings.append(hidden_layers)\n",
    "        concatenated_last_4_layers = [torch.cat((layer[-1], layer[-2], layer[-3], layer[-4]), 0) for layer in token_embeddings] # [number_of_tokens, 3072]\n",
    "        #key = token + tokenid\n",
    "        token_dict[tokenized_text[token_i] + '_' + str(token_i)] = concatenated_last_4_layers[len(concatenated_last_4_layers) - 1]\n",
    "        \n",
    "    # Sanity check of the dimensions:\n",
    "    '''print ('Shape is: %d x %d' % (len(concatenated_last_4_layers), len(concatenated_last_4_layers[0])))\n",
    "    print (\"Number of tokens in sequence:\", len(token_embeddings))\n",
    "    print (\"Number of layers per token:\", len(token_embeddings[0]))'''\n",
    "    return token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def make_embed(v, w_dict, lu_to_word):\n",
    "    \n",
    "    embed = []\n",
    "    problem = []\n",
    "    \n",
    "    for k1, v1 in w_dict.items():\n",
    "        try:\n",
    "            if k1.split('_')[0].replace('##','').lower() == v :\n",
    "                v1 = v1.numpy()\n",
    "                embed.append(v1)\n",
    "\n",
    "            elif k1.split('_')[0].replace('##','').lower() in lu_to_word[v] and len(k1.split('_')[0].replace('##','')) > 2 :\n",
    "                    v1 = v1.numpy()\n",
    "                    embed.append(v1)\n",
    "        except:\n",
    "                continue\n",
    "\n",
    "    if len(embed) > 1:\n",
    "        embed = np.average(embed, axis = 0)\n",
    "\n",
    "    elif len(embed) == 1:\n",
    "        embed = list(embed)[0]\n",
    "\n",
    "    if len(embed) == 0:\n",
    "        problem.append((v, k1, lu_to_word))\n",
    "        embed = list(np.zeros(3072))\n",
    "    \n",
    "    #print(len(embed))\n",
    "        \n",
    "    return embed, problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def preprocessing(sent_list, lu_dict, fe_lu_dict, extra_sent):\n",
    "    \n",
    "    token_dict = {}\n",
    "    token_dict_fe = {}\n",
    "    sent_dict = {}\n",
    "    if extra_sent == 0:\n",
    "        marked_text = ['[CLS] ' + sent[0] + ' [SEP]' for sent in sent_list]\n",
    "        #print(marked_text)\n",
    "        sent_id = {sent[0] : sent[1] for sent in sent_list}\n",
    "    elif extra_sent == 1:\n",
    "        marked_text = ['[CLS] ' + sent + ' [SEP]' for sent in sent_list]\n",
    "    problem = []\n",
    "    problem_fe = []\n",
    "    \n",
    "    for text in marked_text:\n",
    "        tokenized_text = tokenizer.tokenize(text)\n",
    "        #print(tokenized_text)\n",
    "    \n",
    "        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        segments_ids = [1] * len(tokenized_text)\n",
    "        \n",
    "        # Convert inputs to PyTorch tensors\n",
    "        tokens_tensors = torch.tensor([indexed_tokens])\n",
    "        segments_tensors = torch.tensor([segments_ids])\n",
    "        \n",
    "        # Predict hidden states features for each layer\n",
    "        with torch.no_grad():\n",
    "            encoded_layers, _ = model(tokens_tensors, segments_tensors)\n",
    "            \n",
    "        #sentence vector\n",
    "        temp = text.replace('[CLS] ', '')\n",
    "        temp = temp.replace(' [SEP]', '')\n",
    "        sent_dict[temp] = (torch.mean(encoded_layers[11], 1)).numpy()\n",
    "        \n",
    "        if extra_sent == 0 :\n",
    "            #word vectors\n",
    "            w_dict = fetch_token_embedding(encoded_layers, tokenized_text)\n",
    "\n",
    "            lu_list = [v for k, v in lu_dict[sent_id[temp]].items()]\n",
    "            lu_to_word = lu_to_word_mapper(lu_list, temp)\n",
    "            #now key = token + token id + sent id \n",
    "            for k, v in lu_dict[sent_id[temp]].items():\n",
    "                token_dict[v + '_' + k + '_' + sent_id[temp]], prob = make_embed(v, w_dict, lu_to_word)\n",
    "\n",
    "            problem.extend(prob)\n",
    "\n",
    "            try:\n",
    "                fe_lu_list = [v for k, v in fe_lu_dict[sent_id[temp]].items()]\n",
    "                fe_lu_to_word = lu_to_word_mapper(fe_lu_list, temp)\n",
    "                for k, v in fe_lu_dict[sent_id[temp]].items():\n",
    "                    token_dict_fe[v + '_' + k + '_' + sent_id[temp]], prob_fe = make_embed(v, w_dict, lu_to_word)\n",
    "                    problem_fe.extend(prob_fe)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "    if extra_sent == 0:\n",
    "        print(len(problem))\n",
    "        print(len(problem_fe))\n",
    "\n",
    "    if extra_sent == 1:\n",
    "        return sent_dict\n",
    "    elif extra_sent == 0:\n",
    "        return token_dict, token_dict_fe, sent_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "424\n",
      "18\n",
      "1441\n",
      "6\n",
      "93\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()\n",
    "\n",
    "#pre-processing\n",
    "#english\n",
    "lu_en = sentwise_ft('en', lu)\n",
    "fe_lu_en = sentwise_ft('en', frame_element_lu)\n",
    "token_dict_en, token_dict_en_fe, sent_dict_en = preprocessing(sentences_en, lu_en, fe_lu_en, 0)\n",
    "\n",
    "#portuguese\n",
    "lu_pt = sentwise_ft('pt', lu)\n",
    "fe_lu_pt = sentwise_ft('pt', frame_element_lu)\n",
    "token_dict_pt, token_dict_pt_fe, sent_dict_pt = preprocessing(sentences_pt, lu_pt, fe_lu_pt, 0)\n",
    "\n",
    "#deutsche\n",
    "lu_de = sentwise_ft('de', lu)\n",
    "fe_lu_de = sentwise_ft('de', frame_element_lu)\n",
    "token_dict_de, token_dict_de_fe, sent_dict_de = preprocessing(sentences_de, lu_de, fe_lu_de, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_dict_de['gut_29470_1275'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157 265 42\n"
     ]
    }
   ],
   "source": [
    "#for non-frame-annotated sentences \n",
    "print(len(sent_dict_en), len(sent_dict_pt), len(sent_dict_de))\n",
    "extra_sents_en = []\n",
    "extra_sents_pt = []\n",
    "extra_sents_de = []\n",
    "\n",
    "import pandas as pd\n",
    "path_name = \"../resources/en-pt-de.csv\"\n",
    "df = pd.read_csv(path_name, skiprows = 1, names = [\"En_Id\",\"En_Sentence\",\"Pt_Id\",\"Pt_Sentence\",\"De_Id\",\"De_Sentence\"], encoding = 'utf-8')\n",
    "\n",
    "for index, rows in df.iterrows():\n",
    "    if rows['En_Sentence'] not in sent_dict_en:\n",
    "        extra_sents_en.append(rows['En_Sentence'])\n",
    "        \n",
    "    elif rows['Pt_Sentence'] not in sent_dict_pt:\n",
    "        extra_sents_pt.append(rows['Pt_Sentence'])\n",
    "    \n",
    "    elif rows['De_Sentence'] not in sent_dict_de:\n",
    "        extra_sents_de.append(rows['De_Sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161 272 62\n"
     ]
    }
   ],
   "source": [
    "#for non-frame annotated sentences\n",
    "extra_sent_dict_en = preprocessing(extra_sents_en, {}, {}, 1)\n",
    "extra_sent_dict_pt = preprocessing(extra_sents_pt, {}, {}, 1)\n",
    "extra_sent_dict_de = preprocessing(extra_sents_de, {}, {}, 1)\n",
    "\n",
    "for k, v in extra_sent_dict_en.items():\n",
    "    sent_dict_en[k] = v\n",
    "\n",
    "for k, v in extra_sent_dict_pt.items():\n",
    "    sent_dict_pt[k] = v\n",
    "\n",
    "for k, v in extra_sent_dict_de.items():\n",
    "    sent_dict_de[k] = v\n",
    "    \n",
    "print(len(sent_dict_en), len(sent_dict_pt), len(sent_dict_de))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../resources/bert_embeddings.pickle\", \"wb\") as pkl_out:\n",
    "    pkl.dump(token_dict_en, pkl_out)\n",
    "    pkl.dump(sent_dict_en, pkl_out)\n",
    "    pkl.dump(token_dict_pt, pkl_out)\n",
    "    pkl.dump(sent_dict_pt, pkl_out)\n",
    "    pkl.dump(token_dict_de, pkl_out)\n",
    "    pkl.dump(sent_dict_de, pkl_out)\n",
    "    pkl.dump(token_dict_en_fe, pkl_out)\n",
    "    pkl.dump(token_dict_pt_fe, pkl_out)\n",
    "    pkl.dump(token_dict_de_fe, pkl_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsoc19",
   "language": "python",
   "name": "gsoc19"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
